---
title: "Conclusions"
author: "Markus Knoefler"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    toc: true
params:
  n: NA
---

```{r setup, include=FALSE}

library(knitr)
library(rgl)
library(ggplot2)
library(car)
library(Hmisc)
library(psych)
library(tm)
library(stringi)
library(proxy)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(sjPlot)
library(cluster)
library(rpart)
library(rpart.plot)
library(rattle)
library(dplyr)

knit_hooks$set(webgl = hook_webgl)
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)

```
# Einleitung
Diese Arbeit wurde in einem **RStudio-Projet** mit **RMarkdown** und **Shiny** umgesetzt. Die Shiny-App ist lediglich ein Proof of Concept fuer mich um mal das Zusammenspiel zu erproben. Sie besteht lediglich aus einem Schieber, mit dem die Anzahl Beobachtungen fuer einen zufaellig generierten Dataframe einstellen werden kann und einem Button, der dann das RMarkdown rendert (knitted). Der komplette **Code** ist auf [Github](https://github.com/knoeflerm/HWZProject) verfuegbar. Die Arbeit sollte somit fuer jeden **reproduzierbar** sein.
\
Da RMarkdown alles untereinander rendert, ist es etwas schwieirig was denn nun drei Folien pro Tag sind. Oft wird jedoch auch einfach nur R-Output gezeigt der viel Platz benoetigt, jedoch nur ein kleiner Teil wichtig ist. Trotzdem ist es fuer mich spaeter einfacher die Gedanken nachzuvollziehen, wenn ich den ganzen Output habe.

# Tag 1 - Was ist BigData

## Daten an der Uni St.Gallen
* Polystrukturierte Daten
    + Logdaten / Service Calls
        - Koennen privacy-kritischen Inhalt haben
    + WLan-Accespoint Daten
        - Ort, Zeit, Dauer
        - Geraet / OS
        - Downloadmenge
    + KPI's der Entwicklung
        - Tickets
        - Shift left
        - Work in Progress
        - Changesets
        - Failed Builds/Tests
* Fast nur strukturierte Daten
    + Alles rund ums Studium (Addressdaten (mit Photo), Noten, Stundenplaene, Raumbelegungen, ...)

## Smart Data - LogDaten
* Hypothesen erstellen und formulieren was man gerne wissen moechte
    + Immer zum Semesterstart, werden die Applikationen in die Knie gezwungen
    + Probleme treten zyklisch auf
* Definieren welche Daten/Werte es braucht, um die Hypothesen zu beweisen
    + Datum, Zeit, Servicename (REST), aufrufende Applikation, OS, Credentials, Statuswerte des Aufrufes
* Massnahmen ergreifen
    + Beispiele koennten sein, ein Pikettdienst, gezielte efforts in Codestabilitaet/Bugfixing/Refactoring, temporaer mehr Serverpower
    
## Data Mining in Wlan-Daten
* Anomaly detection
    + Kann Hinweis fuer ein kommendes Problem sein
* Association Rule Learning
    + Pruefungs- / Lernzeiten, Mensacrowd, Belegung der Lernplaetze
* Clustering
    + Evtl. legen bestimmte Benutzer- / Geraetegruppen unterschiedliche Verhalten an den Tag
* Classification
    +Navigationssystem fuer freie Plaetze zum lernen oder in der Mensa
* Regression
    + Anhand von Simulationen freie Plaetze in naher Zukunft vorschlagen
* Summarization
    + Geht meiner Meinung nach mit der Regression einher. Nur relevante Daten verwenden muessen, um dem Kunden die fuer ihn interessanten Conclusions visualisieren zu koennen

<https://en.wikipedia.org/wiki/Data_mining>

# Tag 2 - Arbeiten mit R

## Daten einlesen
Es gibt verschiedene Moeglichkeiten Daten zu lesen. Der Readbefehl haengt vom Dateityp der Quelle ab. Die bis zum jetzigen Zeitpunkt fuer uns wichtigsten sind:

* read.table("FullPath/FileName.txt")
* read.csv("FullPath/FileName.csv")
* read.xlsx("FullPath/FileName.xlsx")
* und natuerlich die load() Funktion falls die Daten im R eigenen Format vorliegen.
* weitere sind in der Dokumentation zu finden <https://www.statmethods.net/input/importingdata.html>
\newline

```{r}
load("lib/Umfrage.rda")
```

## Uebersicht der Daten
Es ist ratsam sich anfangs jeweils einen groben Ueberblick ueber die geladenen Daten zu verschaffen. Dazu sind die Befehle **dim**, **summary**, **str** und **head** sehr hilfreich.
\
Alle Befehle in R sind online dokumentiert und koennen mit dem Fragezeichen (z.B. ?head) aufgerufen werden.
```{r}
dim(Umfrage) # gibt Auskunft ueber die Dimension der Daten
summary(Umfrage) # zeigt deskriptive Statistiken an
str(Umfrage) # zeigt die Eigenschaften (Variablen, Columns, ...) an
head(Umfrage,5) # zeigt die ersten x Datensaetze an (Default ist 6)
```

## Dataframe

### Zufallswerte
Im Folgenden wird ein Data Frame mit verschiedenen, rein zufaelligen Werten erzeugt. Entweder wird ein Observations count via **Shiny App** als Parameter ins **RMarkdown** gegeben, ansonsten wird ein Defaultwert genommen.
```{r}
minObservations<-100
subObservations<-10
if (params$n == "NA" | params$n < minObservations) {
  observationCount<-minObservations
} else {
  observationCount<-params$n
}
ids<-seq(1,length=observationCount)
head(ids)
age<-round(runif(observationCount,min=18,max=100))
head(age)
sex<-sample(c("w","m"),size=observationCount,replace=T)
head(sex)
size<-round(runif(observationCount,min=150,max=200))
head(size)
income<-round(runif(observationCount,min=60000,max=180000))
head(income)
education<-sample(c("Grundschule","Lehre","Hochschule"),size=observationCount,replace=T)
head(education)
generatedDataFrame<-data.frame(ids,sex,age,size,income,education)
head(generatedDataFrame,minObservations)
generatedDataFrame$sex.f<-as.factor(generatedDataFrame$sex)
dim(generatedDataFrame)
```

### Duplikate
Es waere sehr verwunderlich, wenn es in diesem Dataframe zwei genau gleiche Datensaetze haette.
```{r}
any(duplicated(generatedDataFrame))
```

### Berechnungen und Veraenderungen im Dataframe
Berechnen des Durchschnittalters pro Geschlecht:
```{r}
tapply(generatedDataFrame$age, generatedDataFrame$sex, mean, na.rm=T)
```

Aus dem Dataframe der **`r nrow(generatedDataFrame)`** Datensaetze hat, die ersten **`r subObservations`** Datensaetze nehmen und daraus einen neuen erzeugen.
```{r}
sampleDf<-generatedDataFrame[1:subObservations,]
sampleDf
```

Einen Dataframe aufsplitten, hier z.B. nach Geschlecht:
```{r}
part.female<- split(sampleDf, sampleDf$sex)
part.female
```

### Subset
Subdatensaetze mit gewissen Kriterien kann man auf verschiedene Arten auslesen, z.B. mit **which**, oder mit dem Befehl **subset**.
```{r}
sampleDf[which(sampleDf$income>=140000),]
subset(sampleDf, sampleDf$income>=140000)
```

### Haeufigkeitstabelle
Eine Haeufigkeitstabelle die aussagt, wieviele Frauen oder Maenner dieselbe Ausbildungsstufe haben. Dies kann mit den Befehlen **table** und **xtabs** bewerkstelligt werden.
```{r}
table(sampleDf$sex,sampleDf$education)
xtabs(~sex + education, data=sampleDf)
```

### Matrix
Der Data Frame wird in eine Matrix konvertiert und mit anderen Spalten- und Zeilennamen versehen. Man beachte den Unterschied der Matrix wenn sie mit dem Befehl as.matrix(), oder data.matrix() konvertiert wurde.
```{r}
columnNames<-c("ID", "Geschl.", "Alter", "Groesse", "Einkommen", "Ausbildung", "Geschl.f")
rowNames<-NULL
for (id in sampleDf$ids) {
 rowNames <- c(rowNames, paste('Obs.', id, collapse = "," ))
}
asMatrix<-as.matrix(sampleDf)
dimnames(asMatrix) <- list(rowNames, columnNames)
asMatrix<-subset(asMatrix,select=-ID)
head(asMatrix, minObservations)
dataMatrix<-data.matrix(sampleDf)
dimnames(dataMatrix) <- list(rowNames, columnNames)
dataMatrix<-subset(dataMatrix, select=-ID)
head(dataMatrix, minObservations)
```

# Tag 3 - Dateninspektion und Visualisierung

## Histogramme und Plots
Einige Plots und Histogramme auf dem originalen Dataframe. Da saemtliche Daten im Dataframe zufaellig erzeugt wurden, sehen die Grafiken und Kurven natuerlich auch willkuerlich und etwas chaotisch aus.

```{r ,echo=TRUE, fig.cap="Fig. 3.1 Histogramm mit Dichte (Normalverteilung)", fig.align="center"}
hist(generatedDataFrame$age, prob=T)
lines(density(generatedDataFrame$age), col="red")
```
Man sieht dem Scatterplot in Fig 3.2 an, dass die Daten zufaellig generiert wurden.
```{r ,echo=TRUE, fig.cap="Fig. 3.2 Scatterplot Einkommen nach Alter", fig.align="center"}
plot(generatedDataFrame$age, generatedDataFrame$income, type="p", main="Scatterplot")
```

```{r ,echo=TRUE, fig.cap="Fig. 3.3 Histogramm Income", fig.align="center"}
hist(generatedDataFrame$income, main="Histogramm")
```

## Boxplot
```{r ,echo=TRUE, fig.cap="Fig. 3.4 Boxplot nach Income", fig.align="center"}
boxplot(generatedDataFrame$income, main="Boxplot")
```
Ein Boxplot beinhaltet fuenf Informationen. Der dicke Strich im Kasten ist der Median, der obere und untere Rand des Kastens zeigen die Quantile Q1 (25% der Werte sind kleiner als dieser Wert) und Q3 (25% der Werte sind groesser als dieser Wert) an, der unterste Strich ist das Minimum und der oberste Strich das Maximum

## Plot einer generierten Matrix
An sich ein simpler Plot der jedoch schoen anzuschauen ist und vorallem die Daten aus einem clever kombinierten Dataframe bezieht. Eine an dieser Stelle zentrale Funktion ist **rbind()** welche jeweils die Zeilen eines Dataframes in einen neuen (oder wieder den urspruenglichen) merged. Dieses Beispiel stammt von <http://rstudio-pubs-static.s3.amazonaws.com/18905_c8e7a77909704e90a4a38cd3e8bc30f9.html>

```{r}
iter <- 10000
p <- runif(iter)
coord <- matrix(c(0, 0), ncol = 1)
dfFeather <- rbind(data.frame(), t(coord))
for (i in 1:iter) {
    if (p[i] <= 0.05) {
        m <- matrix(c(0, 0, 0, 0.16), nrow = 2, ncol = 2)
        const <- matrix(c(0, 0), ncol = 1)
    } else if (p[i] > 0.05 && p[i] <= 0.86) {
        m <- matrix(c(0.85, -0.04, 0.04, 0.85), nrow = 2, ncol = 2)
        const <- matrix(c(0, 1.6), ncol = 1)
    } else if (p[i] > 0.86 && p[i] <= 0.93) {
        m <- matrix(c(0.2, 0.23, -0.26, 0.22), nrow = 2, ncol = 2)
        const <- matrix(c(0, 1.6), ncol = 1)

    } else {
        m <- matrix(c(-0.15, 0.26, 0.28, 0.24), nrow = 2, ncol = 2)
        const <- matrix(c(0, 0.44), ncol = 1)
    }
    coord <- m %*% coord + const
    dfFeather <- rbind(dfFeather, t(coord))
}
plot(x = dfFeather[, 2], y = dfFeather[, 1], plt = c(0, 10, -5, 5), cex = 0.1, asp = 1)
```

# Tag 4 - Hypothesen
Im weiteren werden nicht mehr alle R Code Snippets im Report ersichtlich sein (echo=FALSE). Das vollstaendige Markdown mit R-Code ist im Anhang ersichtlich.

\
Hypothesen dienen der **Management Beratung**, oder sie kommen sogar aus dem Management.

* Am Anfang steht die **Diagnose**. Das Fachthema muss verstanden sein, das Problem genau identifiziert und es muss klar sein welche Daten benoetigt werden.
* Nun kann eine **Hypothese** aufgestellt werden. Das Problem kann als hypothetische Ursache-Wirkung formuliert werden.
* **Analyse** erstellen. Ursache-Wirkung anahnd von einem Datenmodell, Fragebogen erstellen. Entweder existieren bereits brauchbare Daten die gesammelt werden muessen, oder sie muessen erhoben werden.
* Nun kann ein **Entscheid** formuliert werden. Besagt der Test, dass die Hypothese signifikant ist, koennen Handlungsoptionen agbeleitet werden und die Wirkung der Massnahmen (theoretisch) gemessen werden.
Die **Qualitaet** des Entscheids haengt von drei Aspekten ab:
    + **Repraesentativitaet**: Analyse wurde mit der korrekten Zielgruppe durchgefuehrt. Je groesser die Zielgruppe, desto besser
    + **Signifikanz**: Das Resultat ist nicht zufaellig und ist bei einer zweiten Analyse in etwa gleich. Auch hier, je groesser n desto signifikanter
    + **Relevanz**: Das Resultat der Analyse kann einen Entscheid untermauern und es koennen Prognosen gemacht werden.

\
Um etwas mit Daten spielen zu koennen, wird ein Dataframe d mit Zufallszahlen generiert (Copy Paste aus dem Skript).
\
Im folgenden Plot ist der Lohn nach Alter fuer die Branchen Finance, Industry und State und die Laender England, Schweiz und die USA aufgezeigt. Die Geschlechter werden mit unterschiedlichen Formen der Punkte unterschieden.

\
In Fig. 4.1 sind folgende Hypothesen ersichtlich:

* Die Loehne unterscheiden sich in den Laendern und Branchen
* Frauen verdienen mehr als Maenner
* In der CH- und US-Industrie verdienen Maenner weniger je aelter sie werden, in allen anderen Branchen ist es umgekehrt.
```{r echo=FALSE}
set.seed(145) # generiert jedesmal die gleichen Werte
n <- 500 # n ist die Anzahl Beobachtungen
Land <- sample(c("CH", "USA", "GB"), n, replace = TRUE, prob = c(0.6, 0.25, 0.15))
Geschl <- rep("X", n) # Dummy "X"
Branche <- rep("X", n) # Dummy "X"
Lohn <- rep(1, n) # Dummy "1"
Alter <- rep(1, n) # Dummy "1"
d <- data.frame(Land, Geschl, Branche, Alter, Lohn) # erstellt DataFrame

######## Datengenerierung per Land #########################
######### Daten f?r CH ###################
d$Geschl <- ifelse(d$Land == "CH", sample(c("M", "F"), n, replace = TRUE, prob = c(0.65, 0.35)),d$Geschl)
d$Branche <- ifelse(d$Land == "CH", sample(c("Fin", "Ind", "Sta"), n, replace = TRUE, prob = c(0.4,0.35,0.15)),d$Branche)
d$Alter <- ifelse(d$Land == "CH", 20+30*rbeta(n, 5, 5), d$Alter)
##### CH-M?nner
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Fin" & d$Geschl=="M",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Ind" & d$Geschl=="M", -(1+runif(n))*d$Alter+120,d$Lohn)
#d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Sta" & d$Geschl=="M",(1.5+runif(n))*d$Alter-20,d$Lohn)
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Sta" & d$Geschl=="M",10+runif(n)*d$Alter,d$Lohn)
##### CH-Frauen
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Fin" & d$Geschl=="F",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Ind" & d$Geschl=="F",(2+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Sta" & d$Geschl=="F",(1.5+runif(n))*d$Alter,d$Lohn)

######### Daten f?r USA ###################
d$Geschl <- ifelse(d$Land == "USA",sample(c("M","F"), n, replace = TRUE, prob = c(0.45,0.55)),d$Geschl)
d$Branche <- ifelse(d$Land == "USA", sample(c("Fin","Ind","Sta"), n, replace = TRUE, prob = c(0.3,0.3,0.4)),d$Branche)

##### USA-M?nner
d$Alter <- ifelse(d$Land == "USA" & d$Geschl == "M",10+30*rbeta(n,5,5),d$Alter)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Fin" & d$Geschl=="M",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Ind" & d$Geschl=="M", -(1+runif(n))*d$Alter+100,d$Lohn)
#d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Sta" & d$Geschl=="M",(1.5+runif(n))*d$Alter-10,d$Lohn)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Sta" & d$Geschl=="M",(0.5+runif(n))*d$Alter,d$Lohn)
##### USA-Frauen
d$Alter <- ifelse(d$Land == "USA" & d$Geschl == "F",25+40*rbeta(n,5,5),d$Alter)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Fin" & d$Geschl=="F",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Ind" & d$Geschl=="F",(1.2+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Sta" & d$Geschl=="F",(1.5+runif(n))*d$Alter,d$Lohn)

######### Daten f?r GB ###################
d$Geschl <- ifelse(d$Land == "GB",sample(c("M","F"), n, replace = TRUE, prob = c(0.5,0.5)),d$Geschl)
d$Branche <- ifelse(d$Land == "GB", sample(c("Fin","Ind","Sta"), n, replace = TRUE, prob = c(0.35,0.35,0.3)),d$Branche)
d$Alter <- ifelse(d$Land == "GB",20+20*rbeta(n,5,5),d$Alter)
##### GB-M?nner
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Fin" & d$Geschl=="M",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Ind" & d$Geschl=="M", -(1+runif(n))*d$Alter+100,d$Lohn)
#d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Sta" & d$Geschl=="M",(1.5+runif(n))*d$Alter-10,d$Lohn)
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Sta" & d$Geschl=="M",(0.5+runif(n))*d$Alter,d$Lohn)
##### GB-Frauen
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Fin" & d$Geschl=="F",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Ind" & d$Geschl=="F",(1.2+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Sta" & d$Geschl=="F",(1.5+runif(n))*d$Alter,d$Lohn)
```
\
```{r fig.cap="4.1 Verteilung", fig.align="center"}
qplot(Alter, Lohn, data=d, shape=Geschl, color=Geschl,
   facets=Land~Branche, size=I(1),
   main="Lohn nach Alter pro Land und Branche mit Geschlechteraufteilung",
   xlab="Alter", ylab="Lohn")
```
```{r fig.cap="4.2 Density", fig.align="center"}
qplot(Lohn, data=d, geom="density", fill=Land, alpha=I(.5),
   main="Verteilung des Lohns", xlab="Lohn in CHF",
   ylab="Dichte")
```

## Tests

### T-Test / Anova
```{r}
CHAndGBSalaries<-subset(d, d$Land=="GB" | d$Land=="CH")
myData<-data.frame(CHAndGBSalaries$Land, CHAndGBSalaries$Lohn)
myData<-droplevels(myData) # loescht den dritten Level da R den Level "USA" preserved. Fuer den T-Test ist dies nicht zwingend erforderlich
t.test(CHAndGBSalaries$Lohn ~ CHAndGBSalaries$Land, data=CHAndGBSalaries, var.equal=TRUE)
```

Der p-value ist im Promillebereich. Somit ist das Resultat signifikant, was in diesem Falle bedeutet, dass sich die Loehne klar nach Land unterscheiden.
\
Ein Anova-Test mit nur zwei Faktoren entspricht dem T-Test. Man kann dies an den identischen p-values erkennen
```{r}
fit<-aov(Lohn ~ Land, data=CHAndGBSalaries)
summary(fit)
```
Werden nun noch die US Daten mitrein gemischelt, ist das Resultat nicht mehr signifikant da die US und CH Loehne recht aehnlich sind.
```{r}
fit<-aov(Lohn ~ Land, data=myData)
summary(fit)
```

### Chisq-Test
Der XQuadrat Test wird oft verwendet, wenn n unabhaengige Beobachtungen in m verschiedene Kategorien fallen. z.B. Verkehrsunfaelle pro Wochentag gemessen ueber ein Jahr.
```{r}
verkehrsunfaelleProWochentag<-c(80,99,78,89,82,79,53)
chisq.test(verkehrsunfaelleProWochentag)
```
Da der p-value relativ klein ist, kann die Nullhypothese (Die Haeufigkeit der Verkehrsunfaelle pro Wochentag unterscheidet sich NICHT) verworfen werden, es gibt also Unterschiede zwischen den Wochentagen

### Prop-Test
Der Probability Test kann gut bei Umfragen/Experimenten verwendet werden. Man laesst z.B. 200 Personen Bier mit und ohne Alkohol trinken und erfasst wie oft sie richtig getippt haben. In diesem Beispiel wird angenommen, dass 145 Personen richtig lagen.
```{r}
prop.test(x=145,n=200,p=0.5,alt="greater")
```
Das Resultat zeigt, dass sich alkoholfreies und alkoholhaltiges Bier im Geschmack unterscheiden (alternativ Hypothese).

### Wilcox-Test
Der Mann-Whitney-Wilcoxon Test wird angewendet wenn zwei unabhaengige Gruppen von Beobachtungen vorliegen. Die Nullhypothese ist, dass es in den Daten von mtcars zwischen Autos mit automatischem (am = automatic) und geschaltetem Getriebe keinen Unterschied im Verbrauch (mpg = miles per gallon) gibt.
```{r}
wilcox.test(mpg ~ am, data=mtcars)
```
```{r fig.cap="4.3 Verteilung der mpg", fig.align="center"}
qplot(mpg, data=mtcars, geom="density", fill=as.factor(am), alpha=I(.5),
   xlab="Meilen pro Gallone",
   ylab="Dichte")

fit <- aov(mpg ~ cyl, data=mtcars)
summary(fit)

```
In Fig 4.3 kann man gut sehen, dass Autos mit Handschaltung (1) tendenziell weiter kommen mit einer Gallone Treibstoff.

# Tag 5 - Prognosen mittels Regressionsanalyse
## Regression
### Korrelation
Die **Korrelation** von -0.852 zwischen 'Miles per Gallon' und 'Cylinder' ist nicht gerade gross, fuer dieses Beispiel jedoch ausreichend. Die negative Korrelation bedeutet, dass je mehr 'Cylinder', desto weniger weit kommt ein Auto mit einer Gallone.
\ Das Package 'mtcars' ist [hier](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html) beschrieben.
```{r}
carsData<-mtcars
cor.test(carsData$cyl, carsData$mpg)
```

### Lineare Regression
Je naeher der **Adjusted R-Squared** Wert bei Eins ist, desto besser die Guete des Modells. Die Sternchen * geben an wie signifikant das Resultat ist, der Pr(>|t|) Wert ist somit der p-Value. In Fig. 5.1 ist der schlechte R-Squared Wert und auch die nicht gerade berauschende Korrelation zu sehen, man kann jedoch trotzdem erkennen, dass Autos mit vielen Zylindern weniger weit kommen mit einer Gallone Benzin.
\
Das **Regressionsmodell** lautet somit: mpg = 37.88 - 2.88 * cyl
```{r fig.cap="5.1 Lineare Regression Verbrauch und Anzahl Zylinder", fig.align="center"}
fit<-lm(carsData$mpg ~ carsData$cyl)
summary(fit)
plot(carsData$cyl, carsData$mpg)
abline(fit,col="red")
```

### Regression mit mehr als zwei Praediktoren
Hat man mehr als zwei Praediktoren, laesst sich die Regressionsgerade nicht mehr 2-dimensional darstellen. Dies laesst sich jedoch in einem 3D Diagramm darstellen (mpg = Miles per Gallon, wt = Weight, disp = Displacement). Dreht man an der 3D-Grafik etwas herum, kann man das Regressionsmodell mpg = 39.67 - 3.19 * wt - 1.51 * cyl erkennen.
```{r testgl, webgl=TRUE}
fit<-lm(carsData$mpg ~ carsData$wt + carsData$cyl)
summary(fit)
plot3d(x = carsData$wt, y = carsData$cyl, z = carsData$mpg, col=rainbow(1000))
```

### Standardisierte Regressionskoeffizienten
Mit dem Befehl **scale()** koennen Merkmale standardisiert werden. Das heisst es werden alle Einheiten der Masse entfernt. Der Koeffizient gibt dann nur noch die relative Beziehung zu den Verteilungen der Merkmale an.
```{r}
fit<-lm(mpg ~ cyl + wt + disp, data = carsData)
summary(fit)
fit<-lm(scale(mpg) ~ scale(cyl) + scale(wt) + scale(disp), data = carsData)
summary(fit)
```

## Prediction

#### Original Daten
```{r}
carsData
```

#### Vorhersagen
Der Verbrauch des Mazda RX4 sinkt bei gleichem Gewicht, jedoch ist der Hubraum (disp) auf nur vier, anstatt 6 Zylinder verteilt.
```{r}
fitCars<-lm(mpg ~ cyl + wt + disp, data = carsData)
predsCars <- data.frame(cyl=4, wt=2.62, disp=160)
predict(fitCars, newdata=predsCars, interval="prediction")
```

### Korrelations-Analyse
Mit einer Korrelationsanalyse kann man sich erst mal einen Ueberblick verschaffen wie gut oder schlecht die Daten miteinander korrelieren. Je naeher die Werte bei eins, desto besser korrelieren sie. In Fig. 5.2 ist eigentlich links unten und gegen rechts oben zweimal die gleiche Information enthalten, einfach mit vertauschten Achsen. Die Plots sind gespiegelt.
```{r fig.cap="5.2 Simple Korrelationsmatrix", fig.align="center"}
cor(carsData[, c("mpg", "cyl", "disp", "wt")])
pairs(~ mpg + cyl + wt + disp, data = carsData)
```
Fig. 5.3 zeigt einen Korrelations Scatter Plot mit etwas mehr Informationsgehalt. Korrelationskoeffizienten, Histogramme mit Density und die Scatterplots die auch in Fig. 5.2 dargestellt sind. Fuer rcorr kann man auch den type="pearson" verwenden was unterschiedliche Korrelationswerte ergibt.
```{r fig.cap="5.3 Graphische Korrelationsmatrix", fig.align="center"}
carsDataReduced<-data.frame(carsData$mpg, carsData$cyl, carsData$wt, carsData$disp)
carsDataMatrix<-data.matrix(carsDataReduced)
rcorr(carsDataMatrix, type = "pearson")
pairs.panels(carsDataReduced, main="Simple Scatterplot Matrix")
```

### Variablen Selektion
Das Ziel der Variablenselektion ist es, aus einer großen Auswahl von potentiell erklärenden
Variablen in einem Datensatz diejenigen herauszufinden, mit denen ein Regressionsmodell
die Vorhersage der Zielvariablen am besten beschreibt. Die Variablenauswahl erfolgt zum einen aus inhaltlichen Gesichtspunkten und zum anderen mit Hilfe unterschiedlicher statistischer Verfahren. Bekannte Selektionsmethoden sind beispielsweise die Backward-, Forward- oder Stepwise-Verfahren
\
Mit der Funktion **step()** kann in R eine Variablenselektion durchgefuehrt werden. Je kleiner der AIC-Wert, desto mehr Erklaerungsgehalt besitzt das Modell.
```{r}
cars.regression<-lm(carsData$mpg ~ carsData$wt + carsData$cyl + carsData$disp)
cars.step <- step(cars.regression,direction="backward")
```

### Factors as Predictors
R erkennt Faktoren und macht daraus automatisch Predictors (dummy Variablen). Dies ist hier mit cyl.f simuliert. Der Befehl **table** gibt den count pro Zylindertype aus, was man auch rechts unten im summary sehen kann.
```{r}
carsData$cyl.f<-as.factor(carsData$cyl)
levels(carsData$cyl.f)<-c("small", "medium", "big")
table(carsData$cyl) # Display count of each Cylinder type
summary(carsData)
```

### Interaktion von Merkmalen
Am p-value kann man sehen, dass das Gewicht gekoppelt mit der Anzahl Zylinder fuer die sechs und acht Zylinder signifikanter ist.
```{r}
fitCars<-lm(mpg ~ cyl + wt + disp, data = carsData)
step(fitCars)
Reg4<- update(fitCars, ~ + wt:cyl.f) # update fuegt dem Modell eine Variable hinzu (https://www.r-bloggers.com/using-the-update-function-during-variable-selection/)
summary(Reg4)
```

# Tag 6 - Textmining
Text Mining, seltener auch Text Data Mining oder Textual Data Mining, ist ein Buendel von Algorithmus-basierten Analyseverfahren zur Entdeckung von Bedeutungsstrukturen aus un- oder schwachstrukturierten Textdaten. Mit statistischen und linguistischen Mitteln erschließt Text-Mining-Software aus Texten Strukturen, die die Benutzer in die Lage versetzen sollen, Kerninformationen der verarbeiteten Texte schnell zu erkennen. Im Optimalfall liefern Text-Mining-Systeme Informationen, von denen die Benutzer zuvor nicht wissen, ob und dass sie in den verarbeiteten Texten enthalten sind. Bei zielgerichteter Anwendung sind Werkzeuge des Text Mining außerdem in der Lage, Hypothesen zu generieren, diese zu überprüfen und schrittweise zu verfeinern.
\
(Quelle: [Wikipedia](https://de.wikipedia.org/wiki/Text_Mining))

## Text bereinigung
Zuerst muessen die Texte bereinigt werden. Alle "nichtssagenden" Woerter (Stopwords) raus wie auch andere unbenoetigte Zeichen (#, @, ., ?, ...). Auch werden die Woerter ge-clustert und ihrem Wortstamm zugeordnet (Stemming).

### Stemming
Als Stemming (Stammformreduktion, Normalformenreduktion) bezeichnet man im Information Retrieval sowie in der linguistischen Informatik ein Verfahren, mit dem verschiedene morphologische Varianten eines Wortes auf ihren gemeinsamen Wortstamm zurückgeführt werden, z. B. die Deklination von Wortes oder Wörter zu Wort und Konjugation von gesehen oder sah zu seh.
\
(Quelle: [Wikipedia](https://de.wikipedia.org/wiki/Stemming))

### Stopwords
Stopwoerter sind Woerter die sehr haeufig vorkommen und nichts zur Informationsrueckgewinnung beitragen. Sie koennen oft weggelassen werden. Es gibt fixe Listen mit Stopwoertern die man verwenden kann. Eine solche Liste pro Sprache existiert auch in R.
\
(Weitere Informationen: [Wikipedia](https://de.wikipedia.org/wiki/Stoppwort))
\

Aus Fig. 6.1 kann man noch nicht viel Information lesen, nach der Bereinigung jedoch wesentlich besser (Fig. 6.2).
```{r echo=FALSE, fig.cap="6.1 Words Frequency mit Stopwords", fig.align="center"}
wiki <- "http://en.wikipedia.org/wiki/"
titles <- c("St._Gallen")

articles <- character(length(titles))

#not need for a for loop in this example
for (i in 1:length(titles)) {
  articles[i] <- stri_flatten(readLines(stri_paste(wiki, titles[i])), col = " ")
}

cleanCorpus<-function(articlesFunc){
  docs <- Corpus(VectorSource(articles))
  
  docs2 <- tm_map(docs, function(x) stri_replace_all_regex(x, "<.+?>", " "))
  docs3 <- tm_map(docs2, function(x) stri_replace_all_fixed(x, "\t", " "))
  
  docs4 <- tm_map(docs3, PlainTextDocument)
  docs5 <- tm_map(docs4, stripWhitespace)
  docs6 <- tm_map(docs5, removePunctuation)
  docs <- tm_map(docs6, tolower)
  
  return(docs)
}

sgArticles<-articles
sgDocsToPlot<-cleanCorpus(sgArticles)

plotWordFrequency<-function(document, title){
  dtm <- TermDocumentMatrix(document)
  m <- as.matrix(dtm)
  v <- sort(rowSums(m),decreasing=TRUE)
  d <- data.frame(word = names(v),freq=v)
  
  barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main = title,
        ylab = "Word frequencies")

  return(d)
}

plotWordFrequency(sgDocsToPlot, "Most frequent words total")
```
```{r echo=FALSE, fig.cap="6.2 Words Frequency ohne Stopwords", fig.align="center"}
sgDocsToPlot <- tm_map(sgDocsToPlot, removeWords, c(stopwords("english"),"160","wikipedia", "gallen", "edit", "dated",
                                      "articles", "st160gallen", "swiss", "containing", "2009", "one", "accessed"))

d<-plotWordFrequency(sgDocsToPlot, "Most frequent words removed Stopwords")

```

## Wordclouds
### Simple
```{r echo=FALSE, fig.cap="6.3 Wordcloud", fig.align="center"}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), scale = c(6, 0.2))
```

### Bigram/Trigram
Da der Mensch aus einer Reihe von Woertern einen Sinn erkennen kann, muss man das einer Maschine beibringen. Dazu kann man Woerter in Paaren nach Haeufigkeit zusammen darstellen. So kann man den Zusammenhang besser erkennen. Ein **Bigram** macht dies mit zwei Woertern, ein **Trigram** mit drei.

```{r echo=FALSE, fig.cap="6.4 Bigram", fig.align="center"}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
tdm.bigram <- TermDocumentMatrix(sgDocsToPlot, control = list(tokenize = BigramTokenizer))
freq <- sort(rowSums(as.matrix(tdm.bigram)), decreasing = TRUE)
freq.df <- data.frame(word = names(freq), freq= freq)

wordcloud(freq.df$word, freq.df$freq, 
          max.words = 50, 
          random.order = F, 
          colors = brewer.pal(8, "Dark2"), 
          scale = c(4,0.7))
```

### Commonality
Eine Comparison Cloud vergleicht zwei (aehnliche) Datasets. Man kann erkennen was in unserem Fall z.B. die Staedt Zuerich und St.Gallen gemeinsam haben.

```{r echo=FALSE, fig.cap="6.5 Commonality Cloud", fig.align="center"}
titles <- c("Zuerich")

articles <- character(length(titles))

#no need for a for loop in this example
for (i in 1:length(titles)) {
  articles[i] <- stri_flatten(readLines(stri_paste(wiki, titles[i])), col = " ")
}
zhArticles<-articles
sgzh_docs<-c(sgArticles, zhArticles)
sgzh_docs <- VectorSource(sgzh_docs)
sgzh_corpus<-VCorpus(sgzh_docs)
sgzh_clean<-cleanCorpus(sgzh_corpus)

sgzh_clean <- tm_map(sgzh_clean, removeWords, c(stopwords("english"), stopwords("german"), "edit","also","160", "2010", "â\200œ", "â€œ"))

sgzh_tdm <- TermDocumentMatrix(sgzh_clean) 
sgzh_m <- as.matrix(sgzh_tdm)
commonality.cloud(sgzh_m, max.words = 100, colors = "steelblue1", scale = c(6,0.7))
```

### Weitere Grafiken

Eine **Comparison cloud** zeigt die Woerter der beiden Staedte voneinander getrennt in einer Cloud.
\
[Words in Common](https://rpubs.com/williamsurles/316682) stellt die Woerter der beiden Staedt nach Haeufigkeit gegenueber.
\
Ein [Dendogram](https://rpubs.com/gaston/dendrograms) zeigt hirarchisch geclusterte Woerter nach Haeufigkeit in einem Tree auf.

## AI as a Service
Natuerlich gibt es auch diverse AI Services von Kommerziellen Anbietern wie z.B.[Microsoft Azure AI + Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning-studio/), [Google AI](https://cloud.google.com/products/ai/), oder [Amazon SageMaker](https://aws.amazon.com/machine-learning/). Diese Produkte sind auf den ersten Blick sehr simpel, man kommt jedoch wenn die Aufgabe etwas komplexer wird sehr schnell an die Grenzen. Man hat nicht wirklich Einfluss auf das was hinter diesen Drag&Drop Bildchen abgeht.

# Tag 7 - Neuronale Netze und Faktor- / Clusteranalyse
## Neuronale Netze
###Konzepte
Neuronen feuern wenn sie einen gewissen Schwellwert erreicht haben, die sogenannte **Aktivierungs-**, oder **Sigmoidfuntion**. Ein Neuron kann mehrere Eingaenge haben, jedoch hat es jeweils nur einen Ausgang, was entlang der Azons zu den Terminals wieder als Eingang (Dendriten) anderer Neuronen dienen kann. So kann ein neuronales Netz in n **Schichten** aufgebaut sein. Eingangsschicht, Ausgangsschicht und dazwischen n versteckte Schichten (hidden Layers).

### Gewichte
Die Gewichte sind das wichtigste in neuronalen Netzen, damit 'lernen' sie. Das Eingangssignal wird mit dem Gewicht multipliziert. Das Neuron summiert nun alle Werte aller Eingangsignale auf und falls dies den Schwellwert erreicht, 'feuert' das Neuron den Ausgangswert. Die Gewichte werden solange veraendert, bis das Resultat ganz am Ende des Netzes dem entspricht was man erwartet. Dies nennt man **supervised learning**, da man weiss was herauskommen sollte und das System solange lernen laesst, bis Soll- und Istausgabe uebereinstimmen. Wird der Fehler wieder vorne ins Netz gespiesen, nennt man dies **Fehlerrueckfuehrung (Backpropagation)**.
\
Beim **unsupervised learning** wird kein Output vorgegeben. Die Gewichtsveraenderungen erfolgen in Abhaengigkeit der
Aehnlichkeit der Gewichte mit den Inputreizen.

## Faktoranalyse
Ziel der Faktorenanalyse (FA) und der Hauptkomponentenanalyse (HKA) ist eine Variablenreduktion. Es werden nur die Kovarianzen aller Merkmale (meist sind es die Items eines Fragebogens) analysiert. Ziel ist es nicht, die vollständige Varianz der Merkmale aufzuklären, sondern nur ihre gemeinsame Varianz (die Kovarianz). Das Verfahren wird in erster Linie eingesetzt, wenn es darum geht, latente Konstrukte oder Strukturen hinter den Merkmalen zu entdecken.
\
Dazu berechnet man erst einmal die Korrelationsmatrix.

```{r echo=FALSE}
carsData<-mtcars
data(carsData)
coping<-carsData[, ]
cor(coping, use = "complete.obs")
```

In Fig. 7.1 sind alle Werte gegeneinander paarweise aufgeplottet. Man kann sehen, dass nicht alle Werte Sinn machen, vorallendingen die mit wenigen verschiedenen Werten wie z.B. ob es ein V- oder Reihenmotor ist (vs), oder automatik- oder Schaltgetriebe (am).
```{r echo=FALSE, fig.cap="7.1 Pairs", fig.align="center"}
pairs(coping, panel = panel.smooth)
```

Untenstehende Warnings ignoriere ich mal gekonnt, da sie fuer meine Ausfuehrungen irrelevant sind. Die Screeplots in Fig. 7.2 und 7.3 zeigen klar den Knick bei drei faktoren, das Modell empfiehlt jedoch nur zwei Faktoren zu machen.
```{r echo=FALSE, fig.cap="7.2 Parallel Analysis", fig.align="center"}
fa.parallel(coping) #PC ist Hauptkomponentenanalyse, FA Faktoranalyse
```

```{r echo=FALSE, fig.cap="7.3 Scree", fig.align="center"}
scree(coping)
```

Ich habe nun doch drei Faktoren verwendet, da fuer die drei der Eigenwert (ss loadings) jeweils groesser Eins ist. Die **Kommunalitaet** (com) ist der Anteil der gemessenen Varianz der gesamten Varianz aller Variablen. **U2** stellt den Fehler dar, der sollte moeglichst klein sein. In Fig. 7.4 kann man erkennen, dass Variblen mit einem eher grossen u2 rot erscheinen.
```{r echo=FALSE}
Faktor<-fa(coping, nfactors = 3, rotate = "varimax")
Faktor # com ist die Komplexitaet, u2 glaub der Fehler. ss loadings sind auch wichtig
```

## Hauptkomponentenanalyse
Bei der HKA versucht man die vollständige Varianz aller Merkmale mit wenigen Faktoren zu erklären. Sie wird dann eingesetzt, wenn das Ziel darin besteht, die Datenstruktur zu reduzieren. 

```{r echo=FALSE, fig.cap="7.4 Hauptkomponenten", fig.align="center"}
HKA<-principal(coping, nfactors=3, rotate = "varimax")
HKA
fa.diagram(HKA)
```

## Clusteranalyse

```{r echo=FALSE}
#Distanzen berechnen
carsDataSub<-carsData[1:15,c(1,4)]
# dist(carsDataSub, method="euclidean")
# dist(carsDataSub, method="manhattan")
# single_verfahren<-agnes(carsDataSub, stand = T, method = "single")
complete_verfahren<-agnes(carsDataSub, stand = T, method = "complete")
```

Eine Submenge von mtcars wurde mit der Methode 'complete' hierarchisch geclustert. In Fig 7.5 koennen z.B. drei Cluster erkannt werden, je nach Verbrauch und PS. Der Duster und der Cadillac sind sehr PS-starke Autos die sehr viel Sprit verbrauchen. Im Cluster ganz links sind die Autos mit dem geringsten Verbrauch und in der Mitte diejenigen mit mittlerem Verbrauch.
```{r, fig.cap="7.5 Complete Verfahren", fig.align="center"}
plot(complete_verfahren, which.plots = 2, main = "Complete linkage, standardisiert")
```

# Tag 8 - Decision Trees
Entscheidungsbaeume sind geordnete, gerichtete Baeume, die der Darstellung von Entscheidungsregeln dienen. Die grafische Darstellung als Baumdiagramm veranschaulicht hierarchisch aufeinanderfolgende Entscheidungen. Sie haben eine Bedeutung in zahlreichen Bereichen, in denen automatisch klassifiziert wird oder aus Erfahrungswissen formale Regeln hergeleitet oder dargestellt werden.

## Classification Tree
Classification Trees haben ein kategorisch abhaengiges Merkmal.
\
Im Hintergrund wird ein Csv mit Daten des Titanicungluecks eingelesen. Da die Daten nach Klasse (pclass) sortiert sind, wird der Datensatz erst einmal gemischelt (sample()), es werden unbenoetigte Merkmale entfernt und Faktoren mit verstaendlichem Text versehen. Danach werden die gesaeuberten Daten in einen Trainingsdatensatz (80%) und einen Testdatensatz (20%) geteilt.
```{r echo=FALSE}
#https://www.guru99.com/r-decision-trees.html
set.seed(678)
titanic <-read.csv("lib/titanic.csv")
shuffle_index <- sample(1:nrow(titanic))
titanic <- titanic[shuffle_index, ]
clean_titanic <- titanic %>%
select(-c(home.dest, cabin, name, X, ticket)) %>%
#Convert to factor level
	mutate(pclass = factor(pclass, levels = c(1, 2, 3), labels = c('Upper', 'Middle', 'Lower')),
	survived = factor(survived, levels = c(0, 1), labels = c('Died', 'Survived'))) %>%
na.omit()
# glimpse(clean_titanic)
create_train_test <- function(data, size = 0.8, train = TRUE) {
    n_row = nrow(data)
    total_row = size * n_row
    train_sample <- 1: total_row
    if (train == TRUE) {
        return (data[train_sample, ])
    } else {
        return (data[-train_sample, ])
    }
}
data_train <- create_train_test(clean_titanic, 0.8, train = TRUE)
data_test <- create_train_test(clean_titanic, 0.8, train = FALSE)
# prop.table(table(data_train$survived))
# prop.table(table(data_test$survived))
```
```{r}
nrow(data_train)
```
Die Daten werden nun mit dem Befehl **rpart** klassifiziert und mit **rpart.plot** in einem Entscheidungsbaum dargestellt.
```{r fig.cap="8.1 Classification Tree", fig.align="center"}
fit <- rpart(survived~., data = data_train, method = 'class')
rpart.plot(fit, extra = 106)
```

In Fig. 8.1 kann man von der Root beginnend herauslesen

* dass 41% der Leute ueberlebten
* von allen Passagieren 63% Maenner waren wovon 21% ueberlebten
* von den Maennern die ueber 3.5 Jahre alt waren nur 19% ueberlebten
* jedoch ueberlebten 74% der maennlichen Babies.

\
Die Blaetter sind mit Survived (gruen) beschriftet sobald mehr als 50% der Leute an dieser Entscheidungsstelle ueberlebten. Ansonsten ist das Leaf gruen.

\
Rechenbeispiel fuer das Leaf ganz links unten:
\
61% aller Menschen an Board waren Maenner ueber 3.5 Jahre (836 * 0.61 = knapp 510), wovon aber nur 19% ueberlebten (510 * 0.19 = knapp 97).

\
Das Blatt ganz rechts unten zeigt die groesste Ueberlebenswahrscheinlichkeit auf. Frauen, die ein Ticket der mittleren oder oberen Klasse buchten, ueberlebten zu 94% ((836 * 0.22) * 0.94 = knapp 173).

### Prediction
Folgende Confusion Matrix zeigt, dass das Modell korrekt 106 Passagiere als tot vorhersagt, jedoch faelschlicherweise 15 ueberlebende als tot. In der zweiten Zeile sind die vom Modell vorhergesagten Ueberlebenden, wobei das Modell 30 Passagiere als Survivors vorhersagte, obwohl sie gestorben sind.

```{r}
predict_unseen <-predict(fit, data_test, type = 'class')
table_mat <- table(data_test$survived, predict_unseen)
table_mat
```

Die Testgenauigkeit betraegt 78%, das heisst im Testdatensatz wurden 78% der Ueberlebenden richtig erkannt.

```{r}
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
print(paste('Testgenauigkeit', accuracy_Test))
```


Folgende Vorhersage zeigt, dass ich wahrscheinlich gestorben waere.

```{r}
predictPassenger<-data.frame(pclass="Lower", sex="male", age=43, sibsp=2, parch=2, fare=12, embarked="S")
prediction <-predict(fit, predictPassenger, type = 'class')
prediction
```


## Regression Tree
Regression Trees haben ein kontinuierlich abhaengiges Merkmal.
\

```{r fig.cap="8.2 Regression Tree", fig.align="center"}
fit <- rpart(age~., data = data_train, method = 'anova')
rpart.plot(fit)
```

```{r}
mean(data_train$age, na.rm = TRUE)
```

In Fig. 8.2 kann man im Root-Leaf sehen, dass das Durchschnittsalter 30 Jahre betraegt. Eher aeltere Leute konnten sich ein Erstklassticket leisten. Von den Ueberlebenden mit Erstklassticket war das Durchschnittsalter 36, von den gestorbenen 43 was 10% aller Passagiere ausmacht.
\
parch => Anzahl Eltern/Kinder an Board
\
sisbs => Anzahl Geschwister/Lebensgefaehrten
\
Zuunterst das dritte Blatt von links zeigt Frauen die ueberlebt haben, mit null oder einem Partner unterwegs waren und eins bis zwei Kinder hatten. Das Durchschnittsalter dieser Gruppe ist 20.

### Prediction
Zum Vergleich eine Prediction auf dem geschaetzten linearen Modell auf den Trainingsdaten
```{r}
fit<-lm(age~., data_train)
predictFemalePassenger<-data.frame(pclass="Lower", sex="female", survived="Survived", sibsp=1, parch=1, fare=12, embarked="S")
prediction <-predict(fit, predictFemalePassenger, interval="prediction")
prediction
```

In den Testdaten ist das Durchschnittsalter dieser Gruppe mit rund 17 Jahren etwas tiefer. Das Modell ist wohl noch nicht optimal.
```{r}
fit<-lm(age~., data_test)
predictFemalePassenger<-data.frame(pclass="Lower", sex="female", survived="Survived", sibsp=1, parch=1, fare=12, embarked="S")
prediction <-predict(fit, predictFemalePassenger, interval="prediction")
prediction
```
