---
title: "Conclusions"
author: "Markus Knoefler"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    toc: true
params:
  n: NA
---

```{r setup, include=FALSE}
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jdk-11.0.1') #used for RWeka package

library(knitr)
library(rgl)
library(ggplot2)
library(car)
library(Hmisc)
library(psych)
library(tm)
library(stringi)
library(proxy)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(RWeka)

knit_hooks$set(webgl = hook_webgl)
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)

```

# Tag 1 - Was ist BigData

## Daten an der Uni St.Gallen
* Polystrukturierte Daten
    + Logdaten / Service Calls
        - Koennen privacy-kritischen Inhalt haben
    + WLan-Accespoint Daten
        - Ort, Zeit, Dauer
        - Geraet / OS
        - Downloadmenge
    + KPI's der Entwicklung
        - Tickets
        - Shift left
        - Work in Progress
        - Changesets
        - Failed Builds/Tests
* Fast nur strukturierte Daten
    + Alles rund ums Studium (Addressdaten (mit Photo), Noten, Stundenplaene, Raumbelegungen, ...)

## Smart Data - LogDaten
* Hypothesen erstellen und formulieren was man gerne wissen moechte
    + Immer zum Semesterstart, werden die Applikationen in die Knie gezwungen
    + Probleme treten zyklisch auf
* Definieren welche Daten/Werte es braucht, um die Hypothesen zu beweisen
    + Datum, Zeit, Servicename (REST), aufrufende Applikation, OS, Credentials, Statuswerte des Aufrufes
* Massnahmen ergreifen
    + Beispiele koennten sein, ein Pikettdienst, gezielte efforts in Codestabilitaet/Bugfixing/Refactoring, temporaer mehr Serverpower
    
## Data mining in Wlan-Daten
* Anomaly detection
    +Kann Hinweis fuer ein kommendes Problem sein
* Association Rule Learning
    + Pruefungs- / Lernzeiten, Mensacrowd, Belegung der Lernplaetze
* Clustering
    + Evtl. legen bestimmte Benutzer- / Geraetegruppen unterschiedliche Verhalten an den Tag
* Classification
    +Navigationssystem fuer freie Plaetze zum lernen oder in der Mensa
* Regression
    + Anhand von Simulationen freie Plaetze in naher Zukunft vorschlagen
* Summarization
    + Geht meiner Meinung nach mit der Regression einher. Nur relevante Daten verwenden muessen, um dem Kunden die fuer ihn interessanten Conclusions visualisieren zu koennen

<https://en.wikipedia.org/wiki/Data_mining>

# Tag 2 - Technologien

## Daten einlesen
Es gibt verschiedene Moeglichkeiten Daten zu lesen. Der Readbefehl haengt vom Dateityp der Quelle ab. Die bis anhing fuer uns wichtigsten sind:

* read.table("FullPath/FileName.txt")
* read.csv("FullPath/FileName.csv")
* read.xlsx("FullPath/FileName.xlsx")
* und natuerlich die load() Funktion falls die Daten im R eigenen Format vorliegen.
* weitere sind in der Dokumentation zu finden <https://www.statmethods.net/input/importingdata.html>
\newline

```{r}
load("C:/Users/mknoefler/OneDrive/Documents/BigData/R/Umfrage.rda")
```

## Uebersicht der Daten
Es ist ratsam sich anfangs jeweils einen groben Ueberblick ueber die geladenen Daten zu verschaffen. Dazu sind die Befehle **dim**, **summary**, **str** und **head** sehr hilfreich.\newline Alle Befehle in R sind online dokumentiert und koennen mit dem Fragezeichen (z.B. ?head) aufgerufen werden.
```{r}
dim(Umfrage) # gibt Auskunft ueber die Dimension der Daten
summary(Umfrage) # zeigt deskriptive Statistiken an
str(Umfrage) # zeigt die Eigenschaften (Variablen, Columns, ...) an
head(Umfrage,5) # zeigt die ersten 10 Datensaetze an
```

## Dataframe

### Zufallswerte
Im folgenden wird ein Data Frame mit verschiedenen, rein zufaelligen Werten erzeugt. Entweder wird ein Observations count via **Shiny App** als Parameter ins **RMarkdown** gegeben, ansonsten wird ein Defaultwert genommen.
```{r}
minObservations<-10
if (params$n == "NA" | params$n < minObservations) {
  observationCount<-minObservations
} else {
  observationCount<-params$n
}
ids<-seq(1,length=observationCount)
head(ids)
age<-round(runif(observationCount,min=18,max=100))
head(age)
sex<-sample(c("w","m"),size=observationCount,replace=T)
head(sex)
size<-round(runif(observationCount,min=150,max=200))
head(size)
income<-round(runif(observationCount,min=60000,max=180000))
head(income)
education<-sample(c("Grundschule","Lehre","Hochschule"),size=observationCount,replace=T)
head(education)
df<-data.frame(ids,sex,age,size,income,education)
head(df,minObservations)
df$sex.f<-as.factor(df$sex)
dim(df)
head(df)
# df$income.f<-as.factor(df$income,c(0,90000,130000,Inf))
# df
```

### Duplikate
Es waere sehr verwunderlich, wenn es in diesem Dataframe zwei genau gleiche Datensaetze haette.
```{r}
any(duplicated(df))
```

### Berechnungen und Veraenderungen im Dataframe
Berechnen des Durchschnittalters pro Geschlecht:
```{r}
tapply(df$age, df$sex, mean, na.rm=T)
```

Aus dem Dataframe der **`r nrow(df)`** Datensaetze hat, die ersten **`r minObservations`** Datensaetze nehmen und daraus einen neuen erzeugen.
```{r}
sampleDf<-df[1:minObservations,]
sampleDf
```

Einen Dataframe aufsplitten, hier z.B. nach Geschlecht:
```{r}
part.female<- split(sampleDf, sampleDf$sex)
part.female
```

### Subset
Datensaetze mit gewissen Kriterien kann man auf verschiedene Arten auslesen, z.B. mit **which**, oder mit dem Befehl **subset**.
```{r}
sampleDf[which(sampleDf$income>=140000),]
subset(sampleDf, sampleDf$income>=140000)
table(sampleDf$sex, sampleDf$education)
```

### Haeufigkeitstabelle
Eine Haeufigkeitstabelle die aussagt, wieviele Frauen oder Maenner dieselbe Ausbildungsstufe haben. Dies kann mit den Befehlen **table** und **xtabs** bewerkstelligt werden.
```{r}
table(sampleDf$sex,sampleDf$education)
xtabs(~sex + education, data=sampleDf)
```

### Matrix
Der Data Frame wird in eine Matrix konvertiert und mit anderen Spalten- und Zeilennamen versehen. Man beachte den Unterschied der Matrix wenn sie mit dem Befehl as.matrix(), oder data.matrix() konvertiert wurde.
```{r}
columnNames<-c("ID", "Geschl.", "Alter", "Groesse", "Einkommen", "Ausbildung", "Geschl.f")
rowNames<-NULL
for (id in sampleDf$ids) {
 rowNames <- c(rowNames, paste('Obs.', id, collapse = "," ))
}
asMatrix<-as.matrix(sampleDf)
dimnames(asMatrix) <- list(rowNames, columnNames)
asMatrix<-subset(asMatrix,select=-ID)
head(asMatrix, minObservations)
dataMatrix<-data.matrix(sampleDf)
dimnames(dataMatrix) <- list(rowNames, columnNames)
dataMatrix<-subset(dataMatrix, select=-ID)
head(dataMatrix, minObservations)
```

# Tag 3 - Dateninspektion und Visualisierung

## Histogramme und Plots
Einige Plots und Histogramme auf dem originalen Dataframe. Da saemtliche Daten im Dataframe zufaellig erzeugt wurden, sehen die Grafiken und Kurven natuerlich auch willkuerlich und etwas chaotisch aus.

```{r ,echo=TRUE, fig.cap="Fig. 3.1 Histogramm mit Dichte (Normalverteilung)", fig.align="center"}
hist(df$age, prob=T)
lines(density(df$age), col="red")
```

```{r ,echo=TRUE, fig.cap="Fig. 3.2 Scatterplot Einkommen nach Alter", fig.align="center"}
plot(df$age, df$income, type="p", main="Scatterplot")
```

```{r ,echo=TRUE, fig.cap="Fig. 3.3 Scatterplot", fig.align="center"}
plot(df$income, main="Scatterplot")
```

```{r ,echo=TRUE, fig.cap="Fig. 3.4 Histogramm Income", fig.align="center"}
hist(df$income, main="Histogramm")
```

## Boxplot
```{r ,echo=TRUE, fig.cap="Fig. 3.5 Boxplot nach Income", fig.align="center"}
boxplot(df$income, main="Boxplot")
```
Ein Boxplot beinhaltet fuenf Informationen. Der dicke Strich im Kasten ist der Median, der obere und untere Rand des Kastens zeigen die Quantile Q1 (25% der Werte sind kleiner als dieser Wert) und Q3 (25% der Werte sind groesser als dieser Wert) an, der unterste Strich ist das Minimum und der oberste Strich das Maximum

## Plot einer generierten Matrix
An sich ein simpler Plot der jedoch schoen anzuschauen ist und vorallem die Daten aus einem clever kombinierten Dataframe bezieht. Eine an dieser Stelle zentrale Funktion ist **rbind()** welche jeweils die Zeilen eines Dataframes in einen neuen (oder wieder den urspruenglichen) merged. Dieses Beispiel stammt von <http://rstudio-pubs-static.s3.amazonaws.com/18905_c8e7a77909704e90a4a38cd3e8bc30f9.html>

```{r}
iter <- 10000
p <- runif(iter)
coord <- matrix(c(0, 0), ncol = 1)
dfFeather <- rbind(data.frame(), t(coord))
for (i in 1:iter) {
    if (p[i] <= 0.05) {
        m <- matrix(c(0, 0, 0, 0.16), nrow = 2, ncol = 2)
        const <- matrix(c(0, 0), ncol = 1)
    } else if (p[i] > 0.05 && p[i] <= 0.86) {
        m <- matrix(c(0.85, -0.04, 0.04, 0.85), nrow = 2, ncol = 2)
        const <- matrix(c(0, 1.6), ncol = 1)
    } else if (p[i] > 0.86 && p[i] <= 0.93) {
        m <- matrix(c(0.2, 0.23, -0.26, 0.22), nrow = 2, ncol = 2)
        const <- matrix(c(0, 1.6), ncol = 1)

    } else {
        m <- matrix(c(-0.15, 0.26, 0.28, 0.24), nrow = 2, ncol = 2)
        const <- matrix(c(0, 0.44), ncol = 1)
    }
    coord <- m %*% coord + const
    dfFeather <- rbind(dfFeather, t(coord))
}
plot(x = dfFeather[, 2], y = dfFeather[, 1], plt = c(0, 10, -5, 5), cex = 0.1, asp = 1)
```

# Tag 4 - Hypothesen
Im weiteren werden nicht mehr alle R Code Snippets im Report ersichtlich sein (echo=FALSE). Das vollstaendige Markdown mit R-Code ist im Anhang ersichtlich.

\
Hypothesen dienen der **Management Beratung**, oder sie kommen sogar aus dem Management.

* Am Anfang steht die **Diagnose**. Das Fachthema muss verstanden, das Problem genau identifiziert und es muss klar sein welche Daten benoetigt werden.
* Nun kann eine **Hypothese** aufgestellt werden. Das Problem kann als hypothetische Ursache-Wirkung formuliert werden.
* **Analyse** erstellen.Ursache-Wirkung anahnd von einem Datenmodell, Fragebogen erstellen. Entweder existieren bereits brauchbare Daten die gesammelt werden muessen, oder sie muessen erhoben werden.
* Nun kann ein **Entscheid** formuliert werden. Besagt der Test, dass die Hypothese signifikant ist, koennen Handlungsoptionen agbeleitet werden und die Wirkung der Massnahmen (theoretisch) gemessen werden.
Die **Qualitaet** des Entscheids haengt von drei Aspekten ab:
    + **Repraesentativitaet**: Analyse wurde mit der korrekten Zielgruppe durchgefuehrt. Je groesser die Zielgruppe, desto besser
    + **Signifikanz**: Das Resultat ist nicht zufaellig und ist bei einer zweiten Analyse in etwa gleich. Auch hier, je groesser n desto signifikanter
    + **Relevanz**: Das Resultat der Analyse kann einen Entscheid untermauern und es koennen Prognosen gemacht werden.

\
Um etwas mit Daten spielen zu koennen, wird ein Dataframe d mit Zufallszahlen generiert (Copy Paste aus dem Skript).
\
Im folgenden Plot ist der Lohn nach Alter fuer die Branchen Finance, Industry und State und die Laender England, Schweiz und die USA aufgezeigt. Die Geschlechter werden mit unterschiedlichen Formen der Punkte unterschieden.

\
In Fig. 4.1 sind folgende Hypothesen ersichtlich:

* Die Loehne unterscheiden sich in den Laendern und Branchen
* Frauen verdienen mehr als Maenner
* In der CH- und US-Industrie verdienen Maenner weniger je aelter sie werden, in allen anderen Branchen ist es umgekehrt.
```{r echo=FALSE}
set.seed(145) # generiert jedesmal die gleichen Werte
n <- 500 # n ist die Anzahl Beobachtungen
Land <- sample(c("CH", "USA", "GB"), n, replace = TRUE, prob = c(0.6, 0.25, 0.15))
Geschl <- rep("X", n) # Dummy "X"
Branche <- rep("X", n) # Dummy "X"
Lohn <- rep(1, n) # Dummy "1"
Alter <- rep(1, n) # Dummy "1"
d <- data.frame(Land, Geschl, Branche, Alter, Lohn) # erstellt DataFrame

######## Datengenerierung per Land #########################
######### Daten f?r CH ###################
d$Geschl <- ifelse(d$Land == "CH", sample(c("M", "F"), n, replace = TRUE, prob = c(0.65, 0.35)),d$Geschl)
d$Branche <- ifelse(d$Land == "CH", sample(c("Fin", "Ind", "Sta"), n, replace = TRUE, prob = c(0.4,0.35,0.15)),d$Branche)
d$Alter <- ifelse(d$Land == "CH", 20+30*rbeta(n, 5, 5), d$Alter)
##### CH-M?nner
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Fin" & d$Geschl=="M",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Ind" & d$Geschl=="M", -(1+runif(n))*d$Alter+120,d$Lohn)
#d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Sta" & d$Geschl=="M",(1.5+runif(n))*d$Alter-20,d$Lohn)
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Sta" & d$Geschl=="M",10+runif(n)*d$Alter,d$Lohn)
##### CH-Frauen
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Fin" & d$Geschl=="F",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Ind" & d$Geschl=="F",(2+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "CH" & d$Branche == "Sta" & d$Geschl=="F",(1.5+runif(n))*d$Alter,d$Lohn)

######### Daten f?r USA ###################
d$Geschl <- ifelse(d$Land == "USA",sample(c("M","F"), n, replace = TRUE, prob = c(0.45,0.55)),d$Geschl)
d$Branche <- ifelse(d$Land == "USA", sample(c("Fin","Ind","Sta"), n, replace = TRUE, prob = c(0.3,0.3,0.4)),d$Branche)

##### USA-M?nner
d$Alter <- ifelse(d$Land == "USA" & d$Geschl == "M",10+30*rbeta(n,5,5),d$Alter)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Fin" & d$Geschl=="M",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Ind" & d$Geschl=="M", -(1+runif(n))*d$Alter+100,d$Lohn)
#d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Sta" & d$Geschl=="M",(1.5+runif(n))*d$Alter-10,d$Lohn)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Sta" & d$Geschl=="M",(0.5+runif(n))*d$Alter,d$Lohn)
##### USA-Frauen
d$Alter <- ifelse(d$Land == "USA" & d$Geschl == "F",25+40*rbeta(n,5,5),d$Alter)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Fin" & d$Geschl=="F",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Ind" & d$Geschl=="F",(1.2+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "USA" & d$Branche == "Sta" & d$Geschl=="F",(1.5+runif(n))*d$Alter,d$Lohn)

######### Daten f?r GB ###################
d$Geschl <- ifelse(d$Land == "GB",sample(c("M","F"), n, replace = TRUE, prob = c(0.5,0.5)),d$Geschl)
d$Branche <- ifelse(d$Land == "GB", sample(c("Fin","Ind","Sta"), n, replace = TRUE, prob = c(0.35,0.35,0.3)),d$Branche)
d$Alter <- ifelse(d$Land == "GB",20+20*rbeta(n,5,5),d$Alter)
##### GB-M?nner
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Fin" & d$Geschl=="M",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Ind" & d$Geschl=="M", -(1+runif(n))*d$Alter+100,d$Lohn)
#d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Sta" & d$Geschl=="M",(1.5+runif(n))*d$Alter-10,d$Lohn)
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Sta" & d$Geschl=="M",(0.5+runif(n))*d$Alter,d$Lohn)
##### GB-Frauen
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Fin" & d$Geschl=="F",(1+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Ind" & d$Geschl=="F",(1.2+runif(n))*d$Alter,d$Lohn)
d$Lohn <- ifelse(d$Land == "GB" & d$Branche == "Sta" & d$Geschl=="F",(1.5+runif(n))*d$Alter,d$Lohn)
```
\
```{r fig.cap="4.1 Verteilung", fig.align="center"}
qplot(Alter, Lohn, data=d, shape=Geschl, color=Geschl,
   facets=Land~Branche, size=I(1),
   main="Lohn nach Alter pro Land und Branche mit Geschlechteraufteilung",
   xlab="Alter", ylab="Lohn")
```
```{r fig.cap="4.2 Density", fig.align="center"}
qplot(Lohn, data=d, geom="density", fill=Land, alpha=I(.5),
   main="Verteilung des Lohns", xlab="Lohn in CHF",
   ylab="Dichte")
```

## Tests

### T-Test / Anova
```{r}
CHAndGBSalaries<-subset(d, d$Land=="GB" | d$Land=="CH")
myData<-data.frame(CHAndGBSalaries$Land, CHAndGBSalaries$Lohn)
myData<-droplevels(myData) # loescht den dritten Level da R den Level "USA" preserved. Fuer den T-Test ist dies nicht zwingend erforderlich
t.test(CHAndGBSalaries$Lohn ~ CHAndGBSalaries$Land, data=CHAndGBSalaries, var.equal=TRUE)
```

Der p-value ist im Promillebereich. Somit ist das Resultat signifikant, was in diesem Falle bedeutet, dass sich die Loehne klar nach Land unterscheiden.
\
Ein Anova-Test mit nur zwei Faktoren entspricht dem T-Test. Man kann dies an den identischen p-values erkennen
```{r}
fit<-aov(Lohn ~ Land, data=CHAndGBSalaries)
summary(fit)
```
Werden nun noch die US Daten mitrein gemischelt, ist das Resultat nicht mehr signifikant da die US und CH Loehne recht aehnlich sind.
```{r}
fit<-aov(Lohn ~ Land, data=myData)
summary(fit)
```

### Chisq-Test
Der XQuadrat Test wird oft verwendet, wenn n unabhaengige Beobachtungen in m verschiedene Kategorien fallen. z.B. Verkehrsunfaelle pro Wochentag gemessen ueber ein Jahr.
```{r}
verkehrsunfaelleProWochentag<-c(80,99,78,89,82,79,53)
chisq.test(verkehrsunfaelleProWochentag)
```
Da der p-value relativ klein ist, kann die Nullhypothese (Die Haeufigkeit der Verkehrsunfaelle pro Wochentag unterscheidet sich NICHT) verworfen werden, es gibt also Unterschiede zwischen den Wochentagen

### Prop-Test
Der Probability Test kann gut bei Umfragen/Experimenten verwendet werden. Man laesst z.B. 200 Personen Bier mit und ohne Alkohol trinken und erfasst wie oft sie richtig getippt haben. In diesem Beispiel wird angenommen, dass 145 Personen richtig lagen.
```{r}
prop.test(x=145,n=200,p=0.5,alt="greater")
```
Das Resultat zeigt, dass sich alkoholfreies und alkoholhaltiges Bier im Geschmack unterscheiden (alternativ Hypothese).

### Wilcox-Test
Der Mann-Whitney-Wilcoxon Test wird angewendet wenn zwei unabhaengige Gruppen von Beobachtungen vorliegen. Die Nullhypothese ist, dass es in den Daten von mtcars zwischen Autos mit automatischem (am = automatic) und geschaltetem Getriebe keinen Unterschied im Verbrauch (mpg = miles per gallon) gibt.
```{r}
wilcox.test(mpg ~ am, data=mtcars)
```
```{r fig.cap="4.3 Verteilung der mpg", fig.align="center"}
qplot(mpg, data=mtcars, geom="density", fill=as.factor(am), alpha=I(.5),
   xlab="Meilen pro Gallone",
   ylab="Dichte")

fit <- aov(mpg ~ cyl, data=mtcars)
summary(fit)

```
In Fig 4.3 kann man gut sehen, dass Autos mit Handschaltung (1) tendenziell weiter kommen mit einer Gallone Treibstoff.

# Tag 5 - Prognosen mittels Regressionsanalyse
## Regression
### Korrelation
Die **Korrelation** von -0.852 zwischen 'Miles per Gallon' und 'Cylinder' ist nicht gerade gross, fuer dieses Beispiel jedoch ausreichend.
\ Das Package 'mtcars' ist [hier](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html) beschrieben.
```{r}
carsData<-mtcars
cor.test(carsData$cyl, carsData$mpg)
```

### Lineare Regression
Je naeher der **Adjusted R-Squared** Wert bei Eins ist, desto besser die Guete des Modells. Die Sternchen * geben an wie signifikant das Resultat ist, der Pr(>|t|) Wert ist somit der p-Value. In Fig. 5.1 ist der schlechte R-Squared Wert und auch die nicht gerade berauschende Korrelation zu sehen, man kann jedoch trotzdem erkennen, dass Autos mit vielen Zylindern weniger weit kommen mit einer Gallone Benzin.
\
Das **Regressionsmodell** lautet somit: mpg = 37.88 - 2.88 * cyl
```{r fig.cap="5.1 Lineare Regression Verbrauch und Anzahl Zylinder", fig.align="center"}
fit<-lm(carsData$mpg ~ carsData$cyl)
summary(fit)
plot(carsData$cyl, carsData$mpg)
abline(fit,col="red")
```

### Regression mit mehr als zwei Praediktoren
Hat man mehr als zwei Praediktoren, laesst sich die Regressionsgerade nicht mehr 2-dimensional darstellen. Dies laesst sich jedoch in einem 3D Diagramm darstellen (mpg = Miles per Gallon, wt = Weight, disp = Displacement). Dreht man an der 3D-Grafik etwas herum, kann man das Regressionsmodell mpg = 39.67 - 3.19 * wt - 1.51 * cyl erkennen.
```{r testgl, webgl=TRUE}
fit<-lm(carsData$mpg ~ carsData$wt + carsData$cyl)
summary(fit)
plot3d(x = carsData$wt, y = carsData$cyl, z = carsData$mpg, col=rainbow(1000))
```

### Standardisierte Regressionskoeffizienten
Mit dem Befehl **scale()** koennen Merkmale standardisiert werden. Das heisst es werden alle Einheiten der Masse entfernt. Der Koeffizient gibt aber nur noch die relative Beziehung zu den Verteilungen der Merkmale an.
```{r}
fit<-lm(mpg ~ cyl + wt + disp, data = carsData)
summary(fit)
fit<-lm(scale(mpg) ~ scale(cyl) + scale(wt) + scale(disp), data = carsData)
summary(fit)
```

## Prediction

#### Original Daten
```{r}
carsData
```

#### Vorhersagen
Der Verbrauch des Mazda RX4 sinkt bei gleichem Gewicht, jedoch der Hubraum (disp) auf nur vier, anstatt 6 Zylinder verteilt.
```{r}
fitCars<-lm(mpg ~ cyl + wt + disp, data = carsData)
predsCars <- data.frame(cyl=4, wt=2.62, disp=160)
predict(fitCars, newdata=predsCars, interval="prediction")
```

### Korrelations-Analyse
Mit einer Korrelationsanalyse kann man sich erst mal einen Ueberblick verschaffen wie gut oder schlecht die Daten miteinander korrelieren. Je naeher die Werte bei eins, desto besser korrelieren sie. In Fig. 5.2 ist eigentlich links unten und gegen rechts oben zweimal die gleiche Information enthalten, einfach mit vertauschten Achsen.
```{r fig.cap="5.2 Simple Korrelationsmatrix", fig.align="center"}
cor(carsData[, c("mpg", "cyl", "disp", "wt")])
pairs(~ mpg + cyl + wt + disp, data = carsData)
```
Fig. 5.3 zeigt einen Korrelations Scatter Plot mit etwas mehr Informationsgehalt. Korrelationskoeffizienten, Histogramme mit Density und die Scatterplots die auch in Fig. 5.2 dargestellt sind. Fuer rcorr kann man auch den type="pearson" verwenden was unterschiedliche Korrelationswerte ergibt, die ich aber an dieser Stelle nicht erklaeren kann.
```{r fig.cap="5.3 Graphische Korrelationsmatrix", fig.align="center"}
carsDataReduced<-data.frame(carsData$mpg, carsData$cyl, carsData$wt, carsData$disp)
carsDataMatrix<-data.matrix(carsDataReduced)
rcorr(carsDataMatrix, type = "pearson")
pairs.panels(carsDataReduced, main="Simple Scatterplot Matrix")
```

### Variablen Selektion
Das Ziel der Variablenselektion ist es, aus einer großen Auswahl von potentiell erklärenden
Variablen in einem Datensatz diejenigen herauszufinden, mit denen ein Regressionsmodell
die Vorhersage der Zielvariablen am Besten beschreibt. Die Variablenauswahl erfolgt zum einen aus inhaltlichen Gesichtspunkten und zum anderen mit Hilfe unterschiedlicher statistischer Verfahren. Bekannte Selektionsmethoden sind beispielsweise die Backward-, Forward- oder Stepwise-Verfahren
\
Mit der Funktion **step()** kann in R eine Variablenselektion durchgefuehrt werden. Je kleiner der AIC-Wert, desto mehr Erklaerungsgehalt besitzt das Modell.
```{r}
cars.regression<-lm(carsData$mpg ~ carsData$wt + carsData$cyl + carsData$disp)
cars.step <- step(cars.regression,direction="backward")
```

### Factors as Predictors
R erkennt Faktoren und macht daraus automatisch Predictors (dummy Variablen). Dies ist hier mit cyl.f simuliert. Der Befehl **table** gibt den count pro Zylindertype aus, was man auch rechts unten im summary sehen kann.
```{r}
carsData$cyl.f<-as.factor(carsData$cyl)
levels(carsData$cyl.f)<-c("small", "medium", "big")
table(carsData$cyl) # Display count of each Cylinder type
summary(carsData)
```

### Interaktion von Merkmalen
Am p-value kann man sehen, dass das Gewicht gekoppelt mit der Anzahl Zylinder fuer die sechs und acht Zylinder signifikanter ist.
```{r}
fitCars<-lm(mpg ~ cyl + wt + disp, data = carsData)
step(fitCars)
Reg4<- update(fitCars, ~ + wt:cyl.f)
summary(Reg4)
```

# Tag 6 - Textmining
Text Mining, seltener auch Textmining, Text Data Mining oder Textual Data Mining, ist ein Bündel von Algorithmus-basierten Analyseverfahren zur Entdeckung von Bedeutungsstrukturen aus un- oder schwachstrukturierten Textdaten. Mit statistischen und linguistischen Mitteln erschließt Text-Mining-Software aus Texten Strukturen, die die Benutzer in die Lage versetzen sollen, Kerninformationen der verarbeiteten Texte schnell zu erkennen. Im Optimalfall liefern Text-Mining-Systeme Informationen, von denen die Benutzer zuvor nicht wissen, ob und dass sie in den verarbeiteten Texten enthalten sind. Bei zielgerichteter Anwendung sind Werkzeuge des Text Mining außerdem in der Lage, Hypothesen zu generieren, diese zu überprüfen und schrittweise zu verfeinern.
\
(Quelle: [Wikipedia](https://de.wikipedia.org/wiki/Text_Mining))

## Text bereinigung
Zuerst muessen die Texte bereinigt werden. Alle "nichtssagenden" Woerter (Stopwords) raus wie auch andere unbenoetigte Zeichen (#, @, ., ?, ...). Auch werden die Woerter ge-clustert und ihrem Wortstamm zugeordnet (Stemming).

### Stemming
Als Stemming (Stammformreduktion, Normalformenreduktion) bezeichnet man im Information Retrieval sowie in der linguistischen Informatik ein Verfahren, mit dem verschiedene morphologische Varianten eines Wortes auf ihren gemeinsamen Wortstamm zurückgeführt werden, z. B. die Deklination von Wortes oder Wörter zu Wort und Konjugation von gesehen oder sah zu seh.
\
(Quelle: [Wikipedia](https://de.wikipedia.org/wiki/Stemming))

### Stopwords
Stopwoerter sind Woerter die sehr haeufig vorkommen und nichts zur Informationsrueckgewinnung beitragen. Sie koennen oft weggelassen werden. Es gibt fixe Listen mit Stopwoertern die man verwenden kann. Eine solche Liste pro Sprache existiert auch in R.
\
(Weitere Informationen: [Wikipedia](https://de.wikipedia.org/wiki/Stoppwort))
\

Aus Fig. 6.1 kann man noch nicht viel Information lesen, nach der Bereinigung jedoch wesentlich besser (Fig. 6.2).
```{r echo=FALSE, fig.cap="6.1 Words Frequency mit Stopwords", fig.align="center"}
wiki <- "http://en.wikipedia.org/wiki/"
titles <- c("St._Gallen")

articles <- character(length(titles))

#not need for a for loop in this example
for (i in 1:length(titles)) {
  articles[i] <- stri_flatten(readLines(stri_paste(wiki, titles[i])), col = " ")
}

cleanCorpus<-function(articlesFunc){
  docs <- Corpus(VectorSource(articles))
  
  docs2 <- tm_map(docs, function(x) stri_replace_all_regex(x, "<.+?>", " "))
  docs3 <- tm_map(docs2, function(x) stri_replace_all_fixed(x, "\t", " "))
  
  docs4 <- tm_map(docs3, PlainTextDocument)
  docs5 <- tm_map(docs4, stripWhitespace)
  docs6 <- tm_map(docs5, removePunctuation)
  docs <- tm_map(docs6, tolower)
  
  return(docs)
}

sgArticles<-articles
sgDocsToPlot<-cleanCorpus(sgArticles)

plotWordFrequency<-function(document, title){
  dtm <- TermDocumentMatrix(document)
  m <- as.matrix(dtm)
  v <- sort(rowSums(m),decreasing=TRUE)
  d <- data.frame(word = names(v),freq=v)
  
  barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main = title,
        ylab = "Word frequencies")

  return(d)
}

plotWordFrequency(sgDocsToPlot, "Most frequent words total")
```
```{r echo=FALSE, fig.cap="6.2 Words Frequency ohne Stopwords", fig.align="center"}
sgDocsToPlot <- tm_map(sgDocsToPlot, removeWords, c(stopwords("english"),"160","wikipedia", "gallen", "edit", "dated",
                                      "articles", "st160gallen", "swiss", "containing", "2009", "one", "accessed"))

d<-plotWordFrequency(sgDocsToPlot, "Most frequent words removed Stopwords")

```

## Wordclouds
### Simple
```{r echo=FALSE, fig.cap="6.3 Wordcloud", fig.align="center"}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"), scale = c(6, 0.2))
```

### Bigram/Trigram
Da der Mensch aus einer Reihe von Woertern einen Sinn erkennen kann, muss man das einer Maschine beibringen. Dazu kann man Woerter in Paaren nach Haeufigkeit zusammen darstellen. So kann man den Zusammenhang besser erkennen. Ein **Bigram** macht dies mit zwei Woertern, ein **Trigram** mit drei.

```{r echo=FALSE, fig.cap="6.4 Bigram", fig.align="center"}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 2))
tdm.bigram <- TermDocumentMatrix(sgDocsToPlot, control = list(tokenize = BigramTokenizer))
freq <- sort(rowSums(as.matrix(tdm.bigram)), decreasing = TRUE)
freq.df <- data.frame(word = names(freq), freq= freq)

wordcloud(freq.df$word, freq.df$freq, 
          max.words = 50, 
          random.order = F, 
          colors = brewer.pal(8, "Dark2"), 
          scale = c(4,0.7))
```

### Commonality/Comparison Cloud
Eine Comparison Cloud vergleicht zwei (aehnliche) Datasets. Man kann erkennen was in unserem Fall z.B. die Staedt Zuerich und St.Gallen gemeinsam haben.

```{r echo=FALSE, fig.cap="6.5 Commonality Cloud", fig.align="center"}
titles <- c("Zuerich")

articles <- character(length(titles))

#not need for a for loop in this example
for (i in 1:length(titles)) {
  articles[i] <- stri_flatten(readLines(stri_paste(wiki, titles[i])), col = " ")
}
zhArticles<-articles
sgzh_docs<-c(sgArticles, zhArticles)
sgzh_docs <- VectorSource(sgzh_docs)
sgzh_corpus<-VCorpus(sgzh_docs)
sgzh_clean<-cleanCorpus(sgzh_corpus)

sgzh_clean <- tm_map(sgzh_clean, removeWords, c(stopwords("english"), stopwords("german"), "edit","also","160", "2010", "â\200œ", "â€œ"))

sgzh_tdm <- TermDocumentMatrix(sgzh_clean) 
sgzh_m <- as.matrix(sgzh_tdm)
commonality.cloud(sgzh_m, max.words = 100, colors = "steelblue1", scale = c(6,0.7))
```
# ```{r echo=FALSE, fig.cap="6.6 Comparison Cloud", fig.align="center"}
# # could not bring that to run. There are weird characters (â€œ) in ZH Wiki which probably trigger the 'cex' error
# comparison.cloud(sgzh_m, max.words = 6, colors = c("orange", "blue"), scale = c(3,0.5))
# ```

### Words in Common


### Dendogram

## AI as a Service

<!-- # Anhang -->
<!-- ## RMarkdown code -->
<!-- ```{r comment=''} -->
<!-- cat(readLines('Conclusions.Rmd'), sep = '\n') -->
<!-- ``` -->
<!-- ## Shiny app code -->
<!-- ```{r comment=''} -->
<!-- cat(readLines('D:/Projects/HWZ/app.R'), sep = '\n') -->
<!-- ``` -->
